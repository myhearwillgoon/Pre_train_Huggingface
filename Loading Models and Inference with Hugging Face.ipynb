{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading Models and Inference with Hugging Face Inferences**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab explores how to leverage the Hugging Face `transformers` library for various natural language processing (NLP) tasks. It begins by demonstrating text classification and text generation using pretrained models like DistilBERT and GPT-2 without using the `pipeline()` function, covering the steps involved in loading models, tokenizing input, performing inference, and processing outputs. The lab then showcases the simplicity and efficiency of using the `pipeline()` function to accomplish the same tasks with minimal code. By comparing both approaches, the lab illustrates how the `pipeline()` function streamlines the process, making it easier and faster to implement NLP solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: transformers in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: filelock in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if Pytorch installed in local or Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (0.23.0)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torchvision) (2.3.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp313-cp313-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\onedrive - imperial college london\\langchain_llm\\huggingface_fine_tuning\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp313-cp313-win_amd64.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.8/5.5 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 26.9 MB/s  0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp313-cp313-win_amd64.whl (2817.2 MB)\n",
      "   ---------------------------------------- 0.0/2.8 GB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.8 GB 49.0 MB/s eta 0:00:58\n",
      "   ---------------------------------------- 0.0/2.8 GB 21.1 MB/s eta 0:02:13\n",
      "   ---------------------------------------- 0.0/2.8 GB 21.9 MB/s eta 0:02:09\n",
      "   ---------------------------------------- 0.0/2.8 GB 18.5 MB/s eta 0:02:32\n",
      "   ---------------------------------------- 0.0/2.8 GB 24.5 MB/s eta 0:01:55\n",
      "   ---------------------------------------- 0.0/2.8 GB 27.1 MB/s eta 0:01:43\n",
      "   ---------------------------------------- 0.0/2.8 GB 24.4 MB/s eta 0:01:55\n",
      "    --------------------------------------- 0.0/2.8 GB 27.3 MB/s eta 0:01:42\n",
      "    --------------------------------------- 0.0/2.8 GB 26.6 MB/s eta 0:01:45\n",
      "    --------------------------------------- 0.1/2.8 GB 27.1 MB/s eta 0:01:42\n",
      "    --------------------------------------- 0.1/2.8 GB 27.4 MB/s eta 0:01:41\n",
      "    --------------------------------------- 0.1/2.8 GB 26.6 MB/s eta 0:01:44\n",
      "    --------------------------------------- 0.1/2.8 GB 26.8 MB/s eta 0:01:43\n",
      "   - -------------------------------------- 0.1/2.8 GB 26.2 MB/s eta 0:01:45\n",
      "   - -------------------------------------- 0.1/2.8 GB 26.7 MB/s eta 0:01:43\n",
      "   - -------------------------------------- 0.1/2.8 GB 27.1 MB/s eta 0:01:41\n",
      "   - -------------------------------------- 0.1/2.8 GB 25.8 MB/s eta 0:01:46\n",
      "   - -------------------------------------- 0.1/2.8 GB 27.3 MB/s eta 0:01:40\n",
      "   - -------------------------------------- 0.1/2.8 GB 28.9 MB/s eta 0:01:34\n",
      "   - -------------------------------------- 0.1/2.8 GB 28.7 MB/s eta 0:01:35\n",
      "   - -------------------------------------- 0.1/2.8 GB 29.1 MB/s eta 0:01:33\n",
      "   - -------------------------------------- 0.1/2.8 GB 29.8 MB/s eta 0:01:31\n",
      "   - -------------------------------------- 0.1/2.8 GB 29.6 MB/s eta 0:01:31\n",
      "   -- ------------------------------------- 0.1/2.8 GB 29.9 MB/s eta 0:01:30\n",
      "   -- ------------------------------------- 0.2/2.8 GB 29.7 MB/s eta 0:01:30\n",
      "   -- ------------------------------------- 0.2/2.8 GB 30.4 MB/s eta 0:01:28\n",
      "   -- ------------------------------------- 0.2/2.8 GB 30.1 MB/s eta 0:01:29\n",
      "   -- ------------------------------------- 0.2/2.8 GB 30.0 MB/s eta 0:01:29\n",
      "   -- ------------------------------------- 0.2/2.8 GB 30.8 MB/s eta 0:01:26\n",
      "   -- ------------------------------------- 0.2/2.8 GB 30.6 MB/s eta 0:01:26\n",
      "   -- ------------------------------------- 0.2/2.8 GB 30.4 MB/s eta 0:01:27\n",
      "   -- ------------------------------------- 0.2/2.8 GB 30.2 MB/s eta 0:01:27\n",
      "   -- ------------------------------------- 0.2/2.8 GB 30.0 MB/s eta 0:01:28\n",
      "   -- ------------------------------------- 0.2/2.8 GB 30.0 MB/s eta 0:01:28\n",
      "   --- ------------------------------------ 0.2/2.8 GB 29.7 MB/s eta 0:01:28\n",
      "   --- ------------------------------------ 0.2/2.8 GB 29.8 MB/s eta 0:01:28\n",
      "   --- ------------------------------------ 0.2/2.8 GB 29.2 MB/s eta 0:01:29\n",
      "   --- ------------------------------------ 0.2/2.8 GB 28.4 MB/s eta 0:01:32\n",
      "   --- ------------------------------------ 0.2/2.8 GB 27.9 MB/s eta 0:01:34\n",
      "   --- ------------------------------------ 0.2/2.8 GB 28.3 MB/s eta 0:01:32\n",
      "   --- ------------------------------------ 0.2/2.8 GB 27.9 MB/s eta 0:01:33\n",
      "   --- ------------------------------------ 0.2/2.8 GB 27.8 MB/s eta 0:01:33\n",
      "   --- ------------------------------------ 0.2/2.8 GB 28.1 MB/s eta 0:01:32\n",
      "   --- ------------------------------------ 0.3/2.8 GB 28.2 MB/s eta 0:01:31\n",
      "   --- ------------------------------------ 0.3/2.8 GB 27.7 MB/s eta 0:01:33\n",
      "   --- ------------------------------------ 0.3/2.8 GB 27.6 MB/s eta 0:01:33\n",
      "   --- ------------------------------------ 0.3/2.8 GB 27.6 MB/s eta 0:01:33\n",
      "   --- ------------------------------------ 0.3/2.8 GB 28.0 MB/s eta 0:01:31\n",
      "   --- ------------------------------------ 0.3/2.8 GB 28.7 MB/s eta 0:01:29\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 28.4 MB/s eta 0:01:30\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 28.3 MB/s eta 0:01:30\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 28.6 MB/s eta 0:01:29\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 28.1 MB/s eta 0:01:30\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 28.8 MB/s eta 0:01:28\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 28.8 MB/s eta 0:01:27\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 28.8 MB/s eta 0:01:27\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 29.7 MB/s eta 0:01:24\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 29.0 MB/s eta 0:01:26\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 30.7 MB/s eta 0:01:21\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 30.1 MB/s eta 0:01:22\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 29.4 MB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 29.2 MB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 28.7 MB/s eta 0:01:26\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 28.8 MB/s eta 0:01:25\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 28.4 MB/s eta 0:01:26\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 28.4 MB/s eta 0:01:26\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 28.0 MB/s eta 0:01:27\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 27.6 MB/s eta 0:01:28\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 27.1 MB/s eta 0:01:30\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 26.9 MB/s eta 0:01:30\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 27.2 MB/s eta 0:01:29\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 26.6 MB/s eta 0:01:31\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 26.8 MB/s eta 0:01:30\n",
      "   ------ --------------------------------- 0.4/2.8 GB 26.6 MB/s eta 0:01:30\n",
      "   ------ --------------------------------- 0.4/2.8 GB 26.3 MB/s eta 0:01:31\n",
      "   ------ --------------------------------- 0.4/2.8 GB 26.9 MB/s eta 0:01:29\n",
      "   ------ --------------------------------- 0.4/2.8 GB 26.4 MB/s eta 0:01:30\n",
      "   ------ --------------------------------- 0.4/2.8 GB 26.0 MB/s eta 0:01:32\n",
      "   ------ --------------------------------- 0.5/2.8 GB 26.7 MB/s eta 0:01:29\n",
      "   ------ --------------------------------- 0.5/2.8 GB 26.3 MB/s eta 0:01:30\n",
      "   ------ --------------------------------- 0.5/2.8 GB 26.5 MB/s eta 0:01:29\n",
      "   ------ --------------------------------- 0.5/2.8 GB 26.3 MB/s eta 0:01:30\n",
      "   ------ --------------------------------- 0.5/2.8 GB 26.5 MB/s eta 0:01:29\n",
      "   ------ --------------------------------- 0.5/2.8 GB 27.1 MB/s eta 0:01:27\n",
      "   ------ --------------------------------- 0.5/2.8 GB 27.8 MB/s eta 0:01:24\n",
      "   ------- -------------------------------- 0.5/2.8 GB 28.6 MB/s eta 0:01:21\n",
      "   ------- -------------------------------- 0.5/2.8 GB 28.2 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 27.9 MB/s eta 0:01:23\n",
      "   ------- -------------------------------- 0.5/2.8 GB 27.9 MB/s eta 0:01:23\n",
      "   ------- -------------------------------- 0.5/2.8 GB 27.6 MB/s eta 0:01:24\n",
      "   ------- -------------------------------- 0.5/2.8 GB 28.0 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 27.7 MB/s eta 0:01:23\n",
      "   ------- -------------------------------- 0.5/2.8 GB 28.1 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 28.1 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 28.1 MB/s eta 0:01:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 26.2 MB/s eta 0:01:28\n",
      "   ------- -------------------------------- 0.5/2.8 GB 25.9 MB/s eta 0:01:28\n",
      "   ------- -------------------------------- 0.5/2.8 GB 26.1 MB/s eta 0:01:28\n",
      "   ------- -------------------------------- 0.5/2.8 GB 26.1 MB/s eta 0:01:28\n",
      "   ------- -------------------------------- 0.5/2.8 GB 26.1 MB/s eta 0:01:28\n",
      "   ------- -------------------------------- 0.5/2.8 GB 24.4 MB/s eta 0:01:34\n",
      "   ------- -------------------------------- 0.5/2.8 GB 24.0 MB/s eta 0:01:35\n",
      "   ------- -------------------------------- 0.5/2.8 GB 24.0 MB/s eta 0:01:35\n",
      "   ------- -------------------------------- 0.6/2.8 GB 24.6 MB/s eta 0:01:32\n",
      "   -------- ------------------------------- 0.6/2.8 GB 24.3 MB/s eta 0:01:33\n",
      "   -------- ------------------------------- 0.6/2.8 GB 24.1 MB/s eta 0:01:34\n",
      "   -------- ------------------------------- 0.6/2.8 GB 23.6 MB/s eta 0:01:36\n",
      "   -------- ------------------------------- 0.6/2.8 GB 23.9 MB/s eta 0:01:34\n",
      "   -------- ------------------------------- 0.6/2.8 GB 23.4 MB/s eta 0:01:36\n",
      "   -------- ------------------------------- 0.6/2.8 GB 23.2 MB/s eta 0:01:37\n",
      "   -------- ------------------------------- 0.6/2.8 GB 23.2 MB/s eta 0:01:37\n",
      "   -------- ------------------------------- 0.6/2.8 GB 22.9 MB/s eta 0:01:38\n",
      "   -------- ------------------------------- 0.6/2.8 GB 22.6 MB/s eta 0:01:39\n",
      "   -------- ------------------------------- 0.6/2.8 GB 22.7 MB/s eta 0:01:38\n",
      "   -------- ------------------------------- 0.6/2.8 GB 22.6 MB/s eta 0:01:38\n",
      "   -------- ------------------------------- 0.6/2.8 GB 22.1 MB/s eta 0:01:40\n",
      "   -------- ------------------------------- 0.6/2.8 GB 22.8 MB/s eta 0:01:37\n",
      "   -------- ------------------------------- 0.6/2.8 GB 22.7 MB/s eta 0:01:37\n",
      "   --------- ------------------------------ 0.6/2.8 GB 23.0 MB/s eta 0:01:35\n",
      "   --------- ------------------------------ 0.6/2.8 GB 23.2 MB/s eta 0:01:34\n",
      "   --------- ------------------------------ 0.6/2.8 GB 23.2 MB/s eta 0:01:34\n",
      "   --------- ------------------------------ 0.6/2.8 GB 22.9 MB/s eta 0:01:35\n",
      "   --------- ------------------------------ 0.6/2.8 GB 22.9 MB/s eta 0:01:35\n",
      "   --------- ------------------------------ 0.6/2.8 GB 22.9 MB/s eta 0:01:35\n",
      "   --------- ------------------------------ 0.6/2.8 GB 21.9 MB/s eta 0:01:40\n",
      "   --------- ------------------------------ 0.7/2.8 GB 21.3 MB/s eta 0:01:42\n",
      "   --------- ------------------------------ 0.7/2.8 GB 22.3 MB/s eta 0:01:37\n",
      "   --------- ------------------------------ 0.7/2.8 GB 21.9 MB/s eta 0:01:39\n",
      "   --------- ------------------------------ 0.7/2.8 GB 22.1 MB/s eta 0:01:38\n",
      "   --------- ------------------------------ 0.7/2.8 GB 22.1 MB/s eta 0:01:37\n",
      "   --------- ------------------------------ 0.7/2.8 GB 21.9 MB/s eta 0:01:38\n",
      "   --------- ------------------------------ 0.7/2.8 GB 22.3 MB/s eta 0:01:36\n",
      "   --------- ------------------------------ 0.7/2.8 GB 22.0 MB/s eta 0:01:37\n",
      "   --------- ------------------------------ 0.7/2.8 GB 21.8 MB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 21.5 MB/s eta 0:01:39\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 21.6 MB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 21.5 MB/s eta 0:01:39\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 21.8 MB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 21.5 MB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 21.9 MB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 21.7 MB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 21.7 MB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 21.7 MB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 20.5 MB/s eta 0:01:42\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 20.1 MB/s eta 0:01:44\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 20.5 MB/s eta 0:01:42\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 20.6 MB/s eta 0:01:40\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 20.3 MB/s eta 0:01:42\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 20.4 MB/s eta 0:01:41\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 20.6 MB/s eta 0:01:40\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 20.8 MB/s eta 0:01:39\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 20.3 MB/s eta 0:01:41\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 20.4 MB/s eta 0:01:40\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 20.5 MB/s eta 0:01:40\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 20.1 MB/s eta 0:01:41\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 21.3 MB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 21.3 MB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 22.8 MB/s eta 0:01:29\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 22.8 MB/s eta 0:01:29\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 22.4 MB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 22.3 MB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 22.1 MB/s eta 0:01:31\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 22.2 MB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 22.1 MB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 22.1 MB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 22.4 MB/s eta 0:01:29\n",
      "   ------------ --------------------------- 0.9/2.8 GB 22.7 MB/s eta 0:01:27\n",
      "   ------------ --------------------------- 0.9/2.8 GB 23.1 MB/s eta 0:01:25\n",
      "   ------------ --------------------------- 0.9/2.8 GB 23.1 MB/s eta 0:01:25\n",
      "   ------------ --------------------------- 0.9/2.8 GB 22.6 MB/s eta 0:01:27\n",
      "   ------------ --------------------------- 0.9/2.8 GB 23.4 MB/s eta 0:01:23\n",
      "   ------------ --------------------------- 0.9/2.8 GB 23.0 MB/s eta 0:01:25\n",
      "   ------------ --------------------------- 0.9/2.8 GB 22.6 MB/s eta 0:01:26\n",
      "   ------------ --------------------------- 0.9/2.8 GB 22.4 MB/s eta 0:01:27\n",
      "   ------------ --------------------------- 0.9/2.8 GB 21.9 MB/s eta 0:01:28\n",
      "   ------------ --------------------------- 0.9/2.8 GB 22.1 MB/s eta 0:01:27\n",
      "   ------------ --------------------------- 0.9/2.8 GB 21.8 MB/s eta 0:01:28\n",
      "   ------------ --------------------------- 0.9/2.8 GB 21.7 MB/s eta 0:01:29\n",
      "   ------------ --------------------------- 0.9/2.8 GB 21.5 MB/s eta 0:01:30\n",
      "   ------------ --------------------------- 0.9/2.8 GB 21.5 MB/s eta 0:01:30\n",
      "   ------------ --------------------------- 0.9/2.8 GB 21.5 MB/s eta 0:01:30\n",
      "   ------------ --------------------------- 0.9/2.8 GB 20.5 MB/s eta 0:01:34\n",
      "   ------------- -------------------------- 0.9/2.8 GB 22.3 MB/s eta 0:01:26\n",
      "   ------------- -------------------------- 0.9/2.8 GB 22.0 MB/s eta 0:01:27\n",
      "   ------------- -------------------------- 0.9/2.8 GB 21.8 MB/s eta 0:01:27\n",
      "   ------------- -------------------------- 0.9/2.8 GB 21.4 MB/s eta 0:01:29\n",
      "   ------------- -------------------------- 0.9/2.8 GB 21.8 MB/s eta 0:01:27\n",
      "   ------------- -------------------------- 0.9/2.8 GB 21.9 MB/s eta 0:01:26\n",
      "   ------------- -------------------------- 0.9/2.8 GB 21.8 MB/s eta 0:01:27\n",
      "   ------------- -------------------------- 1.0/2.8 GB 21.9 MB/s eta 0:01:26\n",
      "   ------------- -------------------------- 1.0/2.8 GB 21.9 MB/s eta 0:01:26\n",
      "   ------------- -------------------------- 1.0/2.8 GB 21.6 MB/s eta 0:01:26\n",
      "   ------------- -------------------------- 1.0/2.8 GB 21.8 MB/s eta 0:01:25\n",
      "   ------------- -------------------------- 1.0/2.8 GB 21.6 MB/s eta 0:01:26\n",
      "   ------------- -------------------------- 1.0/2.8 GB 21.7 MB/s eta 0:01:26\n",
      "   ------------- -------------------------- 1.0/2.8 GB 21.6 MB/s eta 0:01:26\n",
      "   ------------- -------------------------- 1.0/2.8 GB 21.5 MB/s eta 0:01:26\n",
      "   ------------- -------------------------- 1.0/2.8 GB 21.5 MB/s eta 0:01:26\n",
      "   -------------- ------------------------- 1.0/2.8 GB 21.6 MB/s eta 0:01:25\n",
      "   -------------- ------------------------- 1.0/2.8 GB 22.6 MB/s eta 0:01:21\n",
      "   -------------- ------------------------- 1.0/2.8 GB 22.7 MB/s eta 0:01:21\n",
      "   -------------- ------------------------- 1.0/2.8 GB 22.6 MB/s eta 0:01:21\n",
      "   -------------- ------------------------- 1.0/2.8 GB 22.6 MB/s eta 0:01:20\n",
      "   -------------- ------------------------- 1.0/2.8 GB 22.9 MB/s eta 0:01:19\n",
      "   -------------- ------------------------- 1.0/2.8 GB 23.3 MB/s eta 0:01:17\n",
      "   -------------- ------------------------- 1.0/2.8 GB 22.9 MB/s eta 0:01:18\n",
      "   -------------- ------------------------- 1.0/2.8 GB 23.7 MB/s eta 0:01:15\n",
      "   -------------- ------------------------- 1.1/2.8 GB 23.5 MB/s eta 0:01:16\n",
      "   -------------- ------------------------- 1.1/2.8 GB 23.7 MB/s eta 0:01:15\n",
      "   --------------- ------------------------ 1.1/2.8 GB 24.1 MB/s eta 0:01:13\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.9 MB/s eta 0:01:14\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.8 MB/s eta 0:01:14\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.6 MB/s eta 0:01:14\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.6 MB/s eta 0:01:14\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.5 MB/s eta 0:01:14\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.5 MB/s eta 0:01:14\n",
      "   --------------- ------------------------ 1.1/2.8 GB 24.4 MB/s eta 0:01:11\n",
      "   --------------- ------------------------ 1.1/2.8 GB 24.0 MB/s eta 0:01:12\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.8 MB/s eta 0:01:13\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.7 MB/s eta 0:01:13\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.6 MB/s eta 0:01:13\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.7 MB/s eta 0:01:12\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.4 MB/s eta 0:01:13\n",
      "   --------------- ------------------------ 1.1/2.8 GB 23.4 MB/s eta 0:01:13\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 23.3 MB/s eta 0:01:13\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 22.9 MB/s eta 0:01:14\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 22.6 MB/s eta 0:01:15\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 22.9 MB/s eta 0:01:14\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 22.9 MB/s eta 0:01:14\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 22.2 MB/s eta 0:01:16\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 22.8 MB/s eta 0:01:14\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 22.6 MB/s eta 0:01:14\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 22.3 MB/s eta 0:01:15\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 24.4 MB/s eta 0:01:08\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 24.1 MB/s eta 0:01:09\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 23.7 MB/s eta 0:01:10\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 23.7 MB/s eta 0:01:10\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 23.3 MB/s eta 0:01:11\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 23.5 MB/s eta 0:01:10\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 23.8 MB/s eta 0:01:09\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 23.6 MB/s eta 0:01:09\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 24.7 MB/s eta 0:01:06\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 24.1 MB/s eta 0:01:07\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 24.2 MB/s eta 0:01:07\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 24.4 MB/s eta 0:01:06\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 24.9 MB/s eta 0:01:04\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 24.7 MB/s eta 0:01:05\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 24.1 MB/s eta 0:01:06\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 24.9 MB/s eta 0:01:04\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 24.4 MB/s eta 0:01:05\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 24.3 MB/s eta 0:01:05\n",
      "   ----------------- ---------------------- 1.3/2.8 GB 25.4 MB/s eta 0:01:02\n",
      "   ----------------- ---------------------- 1.3/2.8 GB 24.8 MB/s eta 0:01:03\n",
      "   ----------------- ---------------------- 1.3/2.8 GB 24.4 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.3/2.8 GB 24.5 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.3/2.8 GB 24.0 MB/s eta 0:01:05\n",
      "   ------------------ --------------------- 1.3/2.8 GB 24.1 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.3/2.8 GB 23.9 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.3/2.8 GB 23.4 MB/s eta 0:01:06\n",
      "   ------------------ --------------------- 1.3/2.8 GB 23.7 MB/s eta 0:01:05\n",
      "   ------------------ --------------------- 1.3/2.8 GB 23.7 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.3/2.8 GB 23.6 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.3/2.8 GB 23.9 MB/s eta 0:01:03\n",
      "   ------------------ --------------------- 1.3/2.8 GB 23.5 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.3/2.8 GB 23.3 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.3/2.8 GB 23.5 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.3/2.8 GB 23.5 MB/s eta 0:01:04\n",
      "   ------------------- -------------------- 1.3/2.8 GB 24.3 MB/s eta 0:01:01\n",
      "   ------------------- -------------------- 1.3/2.8 GB 23.9 MB/s eta 0:01:02\n",
      "   ------------------- -------------------- 1.4/2.8 GB 23.8 MB/s eta 0:01:02\n",
      "   ------------------- -------------------- 1.4/2.8 GB 24.2 MB/s eta 0:01:01\n",
      "   ------------------- -------------------- 1.4/2.8 GB 24.0 MB/s eta 0:01:01\n",
      "   ------------------- -------------------- 1.4/2.8 GB 23.6 MB/s eta 0:01:02\n",
      "   ------------------- -------------------- 1.4/2.8 GB 23.6 MB/s eta 0:01:02\n",
      "   ------------------- -------------------- 1.4/2.8 GB 23.7 MB/s eta 0:01:01\n",
      "   ------------------- -------------------- 1.4/2.8 GB 23.8 MB/s eta 0:01:01\n",
      "   ------------------- -------------------- 1.4/2.8 GB 24.9 MB/s eta 0:00:58\n",
      "   ------------------- -------------------- 1.4/2.8 GB 24.3 MB/s eta 0:00:59\n",
      "   ------------------- -------------------- 1.4/2.8 GB 24.6 MB/s eta 0:00:58\n",
      "   ------------------- -------------------- 1.4/2.8 GB 24.2 MB/s eta 0:00:59\n",
      "   ------------------- -------------------- 1.4/2.8 GB 25.1 MB/s eta 0:00:57\n",
      "   -------------------- ------------------- 1.4/2.8 GB 25.7 MB/s eta 0:00:55\n",
      "   -------------------- ------------------- 1.4/2.8 GB 25.1 MB/s eta 0:00:56\n",
      "   -------------------- ------------------- 1.4/2.8 GB 25.0 MB/s eta 0:00:56\n",
      "   -------------------- ------------------- 1.4/2.8 GB 25.8 MB/s eta 0:00:54\n",
      "   -------------------- ------------------- 1.4/2.8 GB 25.8 MB/s eta 0:00:54\n",
      "   -------------------- ------------------- 1.4/2.8 GB 25.1 MB/s eta 0:00:56\n",
      "   -------------------- ------------------- 1.4/2.8 GB 26.0 MB/s eta 0:00:53\n",
      "   -------------------- ------------------- 1.4/2.8 GB 25.5 MB/s eta 0:00:54\n",
      "   -------------------- ------------------- 1.5/2.8 GB 25.5 MB/s eta 0:00:54\n",
      "   -------------------- ------------------- 1.5/2.8 GB 25.1 MB/s eta 0:00:55\n",
      "   -------------------- ------------------- 1.5/2.8 GB 24.9 MB/s eta 0:00:55\n",
      "   -------------------- ------------------- 1.5/2.8 GB 24.9 MB/s eta 0:00:55\n",
      "   -------------------- ------------------- 1.5/2.8 GB 24.3 MB/s eta 0:00:56\n",
      "   -------------------- ------------------- 1.5/2.8 GB 23.6 MB/s eta 0:00:58\n",
      "   -------------------- ------------------- 1.5/2.8 GB 23.9 MB/s eta 0:00:57\n",
      "   --------------------- ------------------ 1.5/2.8 GB 24.7 MB/s eta 0:00:54\n",
      "   --------------------- ------------------ 1.5/2.8 GB 24.2 MB/s eta 0:00:55\n",
      "   --------------------- ------------------ 1.5/2.8 GB 24.1 MB/s eta 0:00:55\n",
      "   --------------------- ------------------ 1.5/2.8 GB 24.7 MB/s eta 0:00:54\n",
      "   --------------------- ------------------ 1.5/2.8 GB 24.9 MB/s eta 0:00:53\n",
      "   --------------------- ------------------ 1.5/2.8 GB 24.8 MB/s eta 0:00:53\n",
      "   --------------------- ------------------ 1.5/2.8 GB 24.7 MB/s eta 0:00:53\n",
      "   --------------------- ------------------ 1.5/2.8 GB 25.3 MB/s eta 0:00:52\n",
      "   --------------------- ------------------ 1.5/2.8 GB 24.9 MB/s eta 0:00:52\n",
      "   --------------------- ------------------ 1.5/2.8 GB 24.6 MB/s eta 0:00:53\n",
      "   --------------------- ------------------ 1.5/2.8 GB 25.2 MB/s eta 0:00:51\n",
      "   --------------------- ------------------ 1.5/2.8 GB 24.8 MB/s eta 0:00:52\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 25.3 MB/s eta 0:00:50\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 25.6 MB/s eta 0:00:49\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 25.0 MB/s eta 0:00:51\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 24.7 MB/s eta 0:00:51\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 25.1 MB/s eta 0:00:50\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 25.7 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 26.0 MB/s eta 0:00:47\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 26.2 MB/s eta 0:00:47\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 25.7 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 26.3 MB/s eta 0:00:46\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 25.6 MB/s eta 0:00:47\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 25.4 MB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 26.3 MB/s eta 0:00:46\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 26.8 MB/s eta 0:00:45\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 26.6 MB/s eta 0:00:45\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 26.1 MB/s eta 0:00:46\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 25.9 MB/s eta 0:00:46\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 26.5 MB/s eta 0:00:44\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 26.1 MB/s eta 0:00:45\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 27.3 MB/s eta 0:00:43\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 26.6 MB/s eta 0:00:44\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 26.2 MB/s eta 0:00:44\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 26.1 MB/s eta 0:00:44\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 26.2 MB/s eta 0:00:44\n",
      "   ------------------------ --------------- 1.7/2.8 GB 26.2 MB/s eta 0:00:43\n",
      "   ------------------------ --------------- 1.7/2.8 GB 26.0 MB/s eta 0:00:44\n",
      "   ------------------------ --------------- 1.7/2.8 GB 26.4 MB/s eta 0:00:43\n",
      "   ------------------------ --------------- 1.7/2.8 GB 26.5 MB/s eta 0:00:42\n",
      "   ------------------------ --------------- 1.7/2.8 GB 26.7 MB/s eta 0:00:42\n",
      "   ------------------------ --------------- 1.7/2.8 GB 28.8 MB/s eta 0:00:39\n",
      "   ------------------------ --------------- 1.7/2.8 GB 27.8 MB/s eta 0:00:40\n",
      "   ------------------------ --------------- 1.7/2.8 GB 28.2 MB/s eta 0:00:39\n",
      "   ------------------------ --------------- 1.7/2.8 GB 28.3 MB/s eta 0:00:39\n",
      "   ------------------------ --------------- 1.7/2.8 GB 28.1 MB/s eta 0:00:39\n",
      "   ------------------------ --------------- 1.7/2.8 GB 28.1 MB/s eta 0:00:39\n",
      "   ------------------------ --------------- 1.7/2.8 GB 26.9 MB/s eta 0:00:40\n",
      "   ------------------------ --------------- 1.8/2.8 GB 27.2 MB/s eta 0:00:40\n",
      "   ------------------------ --------------- 1.8/2.8 GB 27.4 MB/s eta 0:00:39\n",
      "   ------------------------- -------------- 1.8/2.8 GB 26.9 MB/s eta 0:00:40\n",
      "   ------------------------- -------------- 1.8/2.8 GB 27.3 MB/s eta 0:00:39\n",
      "   ------------------------- -------------- 1.8/2.8 GB 26.9 MB/s eta 0:00:39\n",
      "   ------------------------- -------------- 1.8/2.8 GB 26.7 MB/s eta 0:00:40\n",
      "   ------------------------- -------------- 1.8/2.8 GB 26.6 MB/s eta 0:00:40\n",
      "   ------------------------- -------------- 1.8/2.8 GB 26.1 MB/s eta 0:00:40\n",
      "   ------------------------- -------------- 1.8/2.8 GB 27.1 MB/s eta 0:00:38\n",
      "   ------------------------- -------------- 1.8/2.8 GB 27.0 MB/s eta 0:00:38\n",
      "   ------------------------- -------------- 1.8/2.8 GB 27.4 MB/s eta 0:00:37\n",
      "   ------------------------- -------------- 1.8/2.8 GB 27.2 MB/s eta 0:00:37\n",
      "   -------------------------- ------------- 1.8/2.8 GB 28.5 MB/s eta 0:00:35\n",
      "   -------------------------- ------------- 1.8/2.8 GB 28.8 MB/s eta 0:00:34\n",
      "   -------------------------- ------------- 1.9/2.8 GB 28.6 MB/s eta 0:00:34\n",
      "   -------------------------- ------------- 1.9/2.8 GB 29.2 MB/s eta 0:00:33\n",
      "   -------------------------- ------------- 1.9/2.8 GB 30.4 MB/s eta 0:00:31\n",
      "   -------------------------- ------------- 1.9/2.8 GB 30.8 MB/s eta 0:00:31\n",
      "   -------------------------- ------------- 1.9/2.8 GB 31.3 MB/s eta 0:00:30\n",
      "   --------------------------- ------------ 1.9/2.8 GB 32.5 MB/s eta 0:00:28\n",
      "   --------------------------- ------------ 1.9/2.8 GB 32.3 MB/s eta 0:00:28\n",
      "   --------------------------- ------------ 1.9/2.8 GB 32.6 MB/s eta 0:00:28\n",
      "   --------------------------- ------------ 1.9/2.8 GB 33.3 MB/s eta 0:00:27\n",
      "   --------------------------- ------------ 1.9/2.8 GB 35.8 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 2.0/2.8 GB 35.1 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 2.0/2.8 GB 37.0 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 37.5 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 40.5 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 44.7 MB/s eta 0:00:19\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 46.9 MB/s eta 0:00:17\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 55.1 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 55.7 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 55.6 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 56.6 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 57.5 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 55.6 MB/s eta 0:00:13\n",
      "   ------------------------------ --------- 2.1/2.8 GB 58.9 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 2.1/2.8 GB 58.1 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 2.1/2.8 GB 58.1 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 2.2/2.8 GB 58.2 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 2.2/2.8 GB 58.9 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 2.2/2.8 GB 57.8 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 2.2/2.8 GB 60.3 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 2.2/2.8 GB 61.7 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 2.2/2.8 GB 63.1 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 2.2/2.8 GB 60.8 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 2.2/2.8 GB 61.0 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 2.2/2.8 GB 60.0 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 59.2 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 2.3/2.8 GB 58.4 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 2.3/2.8 GB 58.3 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 2.3/2.8 GB 58.1 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 2.3/2.8 GB 57.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 2.3/2.8 GB 57.5 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 2.3/2.8 GB 57.5 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 2.3/2.8 GB 57.7 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 2.4/2.8 GB 57.3 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 2.4/2.8 GB 57.8 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 2.4/2.8 GB 57.8 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 59.7 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 59.9 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 60.2 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 60.7 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 59.9 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 2.5/2.8 GB 60.4 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 62.4 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 64.9 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 64.8 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 64.5 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 64.3 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 2.5/2.8 GB 63.4 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 2.5/2.8 GB 63.4 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 2.6/2.8 GB 62.9 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 2.6/2.8 GB 62.2 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 2.6/2.8 GB 63.4 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 2.6/2.8 GB 63.4 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 2.6/2.8 GB 61.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 2.6/2.8 GB 63.1 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 2.6/2.8 GB 62.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 2.7/2.8 GB 62.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 2.7/2.8 GB 59.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 2.7/2.8 GB 61.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 2.7/2.8 GB 60.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 2.7/2.8 GB 61.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.8 GB 60.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.8 GB 60.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.8 GB 60.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.8 GB 60.2 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 56.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 55.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 55.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 54.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 54.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 55.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 GB 31.6 MB/s  0:01:34\n",
      "Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp313-cp313-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.1/4.1 MB 55.0 MB/s  0:00:00\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\n",
      "  Attempting uninstall: torch\n",
      "\n",
      "    Found existing installation: torch 2.8.0\n",
      "\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "    Uninstalling torch-2.8.0:\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "      Successfully uninstalled torch-2.8.0\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "  Attempting uninstall: torchvision\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "    Found existing installation: torchvision 0.23.0\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "    Uninstalling torchvision-0.23.0:\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "      Successfully uninstalled torchvision-0.23.0\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   ---------------------------------------- 3/3 [torchaudio]\n",
      "\n",
      "Successfully installed torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.57.0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: d:\\OneDrive - Imperial College London\\Langchain_LLM\\Huggingface_Fine_Tuning\\.venv\\Lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_It is recommended that you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - Imperial College London\\Langchain_LLM\\Huggingface_Fine_Tuning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with DistilBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and tokenizer\n",
    "\n",
    "First, let's initialize a tokenizer and a model for sentiment analysis using DistilBERT fine-tuned on the SST-2 dataset. This setup is useful for tasks where you need to quickly classify the sentiment of a piece of text with a pretrained, efficient transformer model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "# \"distilbert-base-uncased-finetuned-sst-2-english\" breakdown:\n",
    "# - distilbert: A smaller, faster version of BERT (50% fewer parameters, 97% of BERT's performance)\n",
    "# - base: The base model size (as opposed to large)\n",
    "# - uncased: Text is converted to lowercase (no distinction between \"Hello\" and \"hello\")\n",
    "# - finetuned: The model has been further trained on a specific task\n",
    "# - sst-2: Stanford Sentiment Treebank v2 dataset (binary sentiment classification)\n",
    "# - english: The model is trained on English text\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "\n",
    "# Load the pre-trained DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the input text\n",
    "Tokenize the input text and convert it to a format suitable for the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 23156,   999,  2017,  1005,  2310,  2180,  1037,  2489,  7281,\n",
      "          2000,  1996, 17094,  1012,  7514,  2663,  2000,  4366,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token ids are the token indexes  ```attention_mask``` is essential for correctly processing padded sequences, ensuring efficient computation, and maintaining model performance. Even when no tokens are explicitly masked, it helps the model differentiate between actual content and padding, which is critical for accurate and efficient processing of input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Perform inference\n",
    "The `torch.no_grad()` context manager is used to disable gradient calculation.\n",
    "This reduces memory consumption and speeds up computation, as gradients are not needed for inference (i.e. when you are not training the model). The **inputs syntax is used to unpack a dictionary of keyword arguments in Python. In the context of the model(**inputs):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-3.9954,  4.3336]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method is `input_ids`, and `attention_mask` is their own parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the logits\n",
    "The logits are the raw, unnormalized predictions of the model. Let's extract the logits from the model's outputs to perform further processing, such as determining the predicted class or calculating probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.9954,  4.3336]]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "\n",
    "print(logits,logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-process the output\n",
    "Convert the logits to probabilities and get the predicted class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert logits to probabilities using softmax\n",
    "# The softmax function transforms raw logits (which can be any real numbers) into a probability distribution \n",
    "# where all values are between 0 and 1 and sum to 1. It does this by exponentiating each logit and then \n",
    "# normalizing by the sum of all exponentiated values, ensuring the output represents valid probabilities.\n",
    "# The argmax function then finds the index of the highest probability, which corresponds to the model's \n",
    "# most confident prediction and represents the predicted class.\n",
    "\n",
    "# dim=-1 applies softmax along the last dimension (columns), normalizing probabilities across all classes for each sample\n",
    "probs = torch.softmax(logits,dim = -1)\n",
    "\n",
    "predicted_class = torch.argmax(probs, dim = -1)\n",
    "predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['Negative', 'Positive']\n",
    "predicted_answer = labels[predicted_class]\n",
    "predicted_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with GPT-2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer\n",
    " Load the pretrained GPT-2 tokenizer. The tokenizer is responsible for converting text into tokens that the model can understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained GPT-2 model with a language modeling head. The model generates text based on the input tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 3bb0f2ab-eecf-4d4c-b2a4-26432f42ee8b)')' thrown while requesting GET https://huggingface.co/gpt2/resolve/main/model.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the input text  \n",
    "Tokenize the input text and convert it to a format suitable for the model, like before you have the token indexes, i.e., inputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the input text\n",
    "# 'pt' stands for PyTorch tensors - this returns the tokenized input as PyTorch tensors instead of lists\n",
    "inputs = tokenizer(prompt,return_tensors='pt')\n",
    "\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform inference  \n",
    "\n",
    " \n",
    " **What is Performance Inference?**\n",
    " \n",
    "Performance inference refers to optimizing the model's inference process to achieve faster generation speeds, lower memory usage, and better resource utilization during text generation. This is crucial when deploying models in production environments where response time and computational efficiency matter.\n",
    " \n",
    " **Why do we need Performance Inference?**\n",
    " 1. **Speed**: Faster text generation improves user experience and allows handling more requests per second\n",
    " 2. **Memory Efficiency**: Reduces GPU/CPU memory usage, allowing larger batch sizes or running on smaller hardware\n",
    "\n",
    "Common performance optimization techniques include using `torch.no_grad()` to disable gradient computation, model quantization, caching mechanisms, and specialized inference engines like ONNX or TensorRT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7454, 2402,  257,  640,   11,  262,  995,  373,  257, 1295,  286, 1049,\n",
       "         8737,  290, 1049, 3514,   13,  383,  995,  373,  257, 1295,  286, 1049,\n",
       "         3514,   11,  290,  262,  995,  373,  257, 1295,  286, 1049, 3514,   13,\n",
       "          383,  995,  373,  257, 1295,  286, 1049, 3514,   11,  290,  262,  995,\n",
       "          373,  257]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text using model.generate() method\n",
    "# This method uses specialized text generation strategies that differ from torch.no_grad():\n",
    "# 1. Autoregressive generation: Generates tokens one by one, feeding previous outputs as inputs\n",
    "# 2. Built-in sampling strategies: Supports greedy search, beam search, top-k, top-p sampling\n",
    "# 3. Generation-specific optimizations: Uses KV-cache to avoid recomputing attention for previous tokens\n",
    "# 4. Automatic stopping criteria: Handles EOS tokens and max length automatically\n",
    "# 5. Memory-efficient decoding: Only stores necessary intermediate states during generation\n",
    "# Unlike torch.no_grad() which just disables gradient computation, generate() implements\n",
    "# sophisticated text generation algorithms optimized for sequential token prediction\n",
    "\n",
    "output_ids = model.generate(\n",
    "    inputs.input_ids, \n",
    "    attention_mask=inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=50, \n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs) \n",
    "\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-process the output  \n",
    "Decode the generated tokens to get the text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face `pipeline()` function\n",
    "\n",
    "The `pipeline()` function from the Hugging Face `transformers` library is a high-level API designed to simplify the usage of pretrained models for various natural language processing (NLP) tasks. It abstracts the complexities of model loading, tokenization, inference, and post-processing, allowing users to perform complex NLP tasks with just a few lines of code.\n",
    "\n",
    "## Definition\n",
    "\n",
    "```python\n",
    "transformers.pipeline(\n",
    "    task: str,\n",
    "    model: Optional = None,\n",
    "    config: Optional = None,\n",
    "    tokenizer: Optional = None,\n",
    "    feature_extractor: Optional = None,\n",
    "    framework: Optional = None,\n",
    "    revision: str = 'main',\n",
    "    use_fast: bool = True,\n",
    "    model_kwargs: Dict[str, Any] = None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- **task**: `str`\n",
    "  - The task to perform, such as \"text-classification\", \"text-generation\", \"question-answering\", etc.\n",
    "  - Example: `\"text-classification\"`\n",
    "\n",
    "- **model**: `Optional`\n",
    "  - The model to use. This can be a string (model identifier from Hugging Face model hub), a path to a directory containing model files, or a pre-loaded model instance.\n",
    "  - Example: `\"distilbert-base-uncased-finetuned-sst-2-english\"`\n",
    "\n",
    "- **config**: `Optional`\n",
    "  - The configuration to use. This can be a string, a path to a directory, or a pre-loaded config object.\n",
    "  - Example: `{\"output_attentions\": True}`\n",
    "\n",
    "- **tokenizer**: `Optional`\n",
    "  - The tokenizer to use. This can be a string, a path to a directory, or a pre-loaded tokenizer instance.\n",
    "  - Example: `\"bert-base-uncased\"`\n",
    "\n",
    "- **feature_extractor**: `Optional`\n",
    "  - The feature extractor to use for tasks that require it (e.g., image processing).\n",
    "  - Example: `\"facebook/detectron2\"`\n",
    "\n",
    "- **framework**: `Optional`\n",
    "  - The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. If not specified, it will be inferred.\n",
    "  - Example: `\"pt\"`\n",
    "\n",
    "- **revision**: `str`, default `'main'`\n",
    "  - The specific model version to use (branch, tag, or commit hash).\n",
    "  - Example: `\"v1.0\"`\n",
    "\n",
    "- **use_fast**: `bool`, default `True`\n",
    "  - Whether to use the fast version of the tokenizer if available.\n",
    "  - Example: `True`\n",
    "\n",
    "- **model_kwargs**: `Dict[str, Any]`, default `None`\n",
    "  - Additional keyword arguments passed to the model during initialization.\n",
    "  - Example: `{\"output_hidden_states\": True}`\n",
    "\n",
    "- **kwargs**: `Any`\n",
    "  - Additional keyword arguments passed to the pipeline components.\n",
    "\n",
    "## Task types\n",
    "\n",
    "The `pipeline()` function supports a wide range of NLP tasks. Here are some of the common tasks:\n",
    "\n",
    "1. **Text Classification**: `text-classification`\n",
    "   - **Purpose**: Classify text into predefined categories.\n",
    "   - **Use Cases**: Sentiment analysis, spam detection, topic classification.\n",
    "\n",
    "2. **Text Generation**: `text-generation`\n",
    "   - **Purpose**: Generate coherent text based on a given prompt.\n",
    "   - **Use Cases**: Creative writing, dialogue generation, story completion.\n",
    "\n",
    "3. **Question Answering**: `question-answering`\n",
    "   - **Purpose**: Answer questions based on a given context.\n",
    "   - **Use Cases**: Building Q&A systems, information retrieval from documents.\n",
    "\n",
    "4. **Named Entity Recognition (NER)**: `ner` (or `token-classification`)\n",
    "   - **Purpose**: Identify and classify named entities (like people, organizations, locations) in text.\n",
    "   - **Use Cases**: Extracting structured information from unstructured text.\n",
    "\n",
    "5. **Summarization**: `summarization`\n",
    "   - **Purpose**: Summarize long pieces of text into shorter, coherent summaries.\n",
    "   - **Use Cases**: Document summarization, news summarization.\n",
    "\n",
    "6. **Translation**: `translation_xx_to_yy` (e.g., `translation_en_to_fr`)\n",
    "   - **Purpose**: Translate text from one language to another.\n",
    "   - **Use Cases**: Language translation, multilingual applications.\n",
    "\n",
    "7. **Fill-Mask**: `fill-mask`\n",
    "   - **Purpose**: Predict masked words in a sentence (useful for masked language modeling).\n",
    "   - **Use Cases**: Language modeling tasks, understanding model predictions.\n",
    "\n",
    "8. **Zero-Shot Classification**: `zero-shot-classification`\n",
    "   - **Purpose**: Classify text into categories without needing training data for those categories.\n",
    "   - **Use Cases**: Flexible and adaptable classification tasks.\n",
    "\n",
    "9. **Feature Extraction**: `feature-extraction`\n",
    "   - **Purpose**: Extract hidden state features from text.\n",
    "   - **Use Cases**: Downstream tasks requiring text representations, such as clustering, similarity, or further custom model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Text classification using `pipeline()`\n",
    "\n",
    "In this example, you will use the `pipeline()` function to perform text classification. You will load a pretrained text classification model and use it to classify a sample text.\n",
    "\n",
    "#### Load the text classification model:\n",
    "We initialize the pipeline for the `text-classification` task, specifying the model `\"distilbert-base-uncased-finetuned-sst-2-english\"`. This model is fine-tuned for sentiment analysis.\n",
    "\n",
    "#### Classify the sample text:\n",
    "We use the classifier to classify a sample text: \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\" The `classifier` function returns the classification result, which is then printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997586607933044}]\n"
     ]
    }
   ],
   "source": [
    "# Load a general text classification model\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Classify a sample text\n",
    "result = classifier(\"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "\n",
    "The output will be a list of dictionaries, where each dictionary contains:\n",
    "\n",
    "- `label`: The predicted label (e.g., \"POSITIVE\" or \"NEGATIVE\").\n",
    "- `score`: The confidence score for the prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Language detection using `pipeline()`\n",
    "\n",
    "In this example, you will use the `pipeline()` function to perform language detection. You will load a pretrained language detection model and use it to identify the language of a sample text.\n",
    "\n",
    "#### Load the language detection model:\n",
    "We initialize the pipeline for the `text-classification` task, specifying the model `\"papluca/xlm-roberta-base-language-detection\"`. This model is fine-tuned for language detection.\n",
    "\n",
    "#### Classify the sample text:\n",
    "We use the classifier to detect the language of a sample text: \"Bonjour, comment ça va?\" The `classifier` function returns the classification result, which is then printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8884902000427246}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = \"Bonjour, comment ça va?\"\n",
    "\n",
    "result2 = classifier(text2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'fr', 'score': 0.9934879541397095}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "result = classifier(\"Bonjour, comment ça va?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "The output will be a list of dictionaries, where each dictionary contains:\n",
    "\n",
    "- `label`: The predicted language label (e.g., \"fr\" for French).\n",
    "- `score`: The confidence score for the prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Text generation using `pipeline()`\n",
    "\n",
    "In this example, you will use the `pipeline()` function to perform text generation. You will load a pretrained text generation model and use it to generate text based on a given prompt.\n",
    "\n",
    "#### Initialize the text generation model:\n",
    "We initialize the pipeline for the `text-generation` task, specifying the model `\"gpt2\"`. GPT-2 is a well-known model for text generation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text generation pipeline with GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text based on a given prompt:\n",
    "We use the generator to generate text based on a prompt: \"Once upon a time\". Let's specify `max_length=50`, `truncation=True` to limit the generated text to 50 tokens and `num_return_sequences=1` to generate one sequence. The `generator` function returns the generated text, which is then printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text based on a given prompt\n",
    "prompt = \"Once upon a time\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "The output will be a list of dictionaries, where each dictionary contains:\n",
    "\n",
    "- `generated_text`: The generated text based on the input prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Text generation using T5 with `pipeline()`\n",
    "\n",
    "In this example, you will use the `pipeline()` function to perform text-to-text generation with the T5 model. You will load a pretrained T5 model and use it to translate a sentence from English to French based on a given prompt.\n",
    "\n",
    "#### Initialize the text generation model:\n",
    "We initialize the pipeline for the `text2text-generation task, specifying the model \"t5-small\". T5 is a versatile model that can perform various text-to-text generation tasks, including translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text generation pipeline with T5\n",
    "generator = pipeline(\"text2text-generation\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text based on a given prompt:\n",
    "We use the generator to translate a sentence from English to French based on the prompt: \"translate English to French: How are you?\". Let's specify `max_length=50` to limit the generated text to 50 tokens and `num_return_sequences=1` to generate one sequence. The `generator` function returns the translated text, which is then printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text based on a given prompt\n",
    "prompt = \"translate English to French: How are you?\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "The output will be a list of dictionaries, where each dictionary contains:\n",
    "\n",
    "- `generated_text`: The generated text based on the input prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Fill-mask task using BERT with `pipeline()`\n",
    "\n",
    "In this exercise, you will use the `pipeline()` function to perform a fill-mask task using the BERT model. You will load a pretrained BERT model and use it to predict the masked word in a given sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introductions\n",
    "\n",
    "1. **Initialize the fill-mask pipeline** with the BERT model.\n",
    "2. **Create a prompt** with a masked token.\n",
    "3. **Generate text** by filling in the masked token.\n",
    "4. **Print the generated text** with the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9584370255470276, 'token': 3007, 'token_str': 'capital', 'sequence': 'the capital of australia is canberra'}\n"
     ]
    }
   ],
   "source": [
    "# Generated masked text\n",
    "masked_text = \"The [MASK] of Australia is Canberra\"\n",
    "\n",
    "mask_filler = pipeline('fill-mask', model = 'bert-base-uncased')\n",
    "result3 = mask_filler(masked_text)\n",
    "print(result3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "## Benefits of using `pipeline()`\n",
    "\n",
    "- **Reduced Boilerplate Code**: Simplifies the code required to perform NLP tasks.\n",
    "- **Improved Readability**: Makes code more readable and expressive.\n",
    "- **Time Efficiency**: Saves time by handling model loading, tokenization, inference, and post-processing automatically.\n",
    "- **Consistent API**: Provides a consistent API across different tasks, allowing for easy experimentation and quick prototyping.\n",
    "- **Automatic Framework Handling**: Automatically handles the underlying framework (TensorFlow or PyTorch).\n",
    "\n",
    "## When to use `pipeline()`\n",
    "\n",
    "- **Quick Prototyping**: When you need to quickly prototype an NLP application or experiment with different models.\n",
    "- **Simple Tasks**: When performing simple or common NLP tasks that are well-supported by the `pipeline()` function.\n",
    "- **Deployment**: When deploying NLP models in environments where simplicity and ease of use are crucial.\n",
    "\n",
    "## When to avoid `pipeline()`\n",
    "\n",
    "- **Custom Tasks**: When you need to perform highly customized tasks that are not well-supported by the `pipeline()` function.\n",
    "- **Performance Optimization**: When you need fine-grained control over the model and tokenization process for performance optimization or specific use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "prev_pub_hash": "eb18238e7406fbacf5ce626c9879729f1ad6c0be47d296de029327dabf4104ed"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
